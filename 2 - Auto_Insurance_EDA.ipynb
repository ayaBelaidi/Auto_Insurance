{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= 'https://www.bbds.ma/wp-content/uploads/2021/04/logo.jpg' width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Guide  \n",
    "------------  \n",
    "- [Project Overview](#project-overview)  \n",
    "- [Part 1: Reading Data - Exploratory Data Analysis with Pandas](#I)\n",
    "- [Part 2: Visual data analysis in Python](#II)\n",
    "- [Part 3: Data Pre-processing &  Preparation](#III)\n",
    "- [Part 4: Predictive Analytics](#IV)\n",
    "- [Part 5: Optimization (Hyper Parameter Tuning)](#V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "Roadmap for Building Machine Learning Models\n",
    "</summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "    1. Prepare Problem  \n",
    "    a) Define The Business Objective  \n",
    "    b) Select the datasets  \n",
    "    c) Load dataset  \n",
    "    d) Load libraries  \n",
    "\n",
    "\n",
    "**Data Pre-processing**  \n",
    "This is the first step in building a machine learning model. Data pre-processing refers to the transformation of data\n",
    "before feeding it into the model. It deals with the techniques that are used to convert unusable raw data into clean \n",
    "reliable data.  \n",
    "  \n",
    "Since data collection is often not performed in a controlled manner, raw data often contains outliers \n",
    "(for example, age = 120), nonsensical data combinations (for example, model: bicycle, type: 4-wheeler), missing values, \n",
    "scale problems, and so on. Because of this, raw data cannot be fed into a machine learning model because it might \n",
    "compromise the quality of the results. As such, this is the most important step in the process of data science.  \n",
    "  \n",
    "\n",
    "    2. Summarize Data  \n",
    "    a) Descriptive statistics  \n",
    "    b) Data visualizations  \n",
    "\n",
    "    3. Prepare Data  \n",
    "    a) Data Cleaning  \n",
    "    b) Feature Selection  \n",
    "    c) Data Transformation  \n",
    "\n",
    "**Model Learning**  \n",
    "After pre-processing the data and splitting it into train/test sets (more on this later), we move on to modeling. Models \n",
    "are nothing but sets of well-defined methods called algorithms that use pre-processed data to learn patterns, which can \n",
    "later be used to make predictions. There are different types of learning algorithms, including supervised, semi-supervised, \n",
    "unsupervised, and reinforcement learning. These will be discussed later.\n",
    "  \n",
    "    4. Modeling Strategy  \n",
    "    a) Select Suitable Algorithms  \n",
    "    b) Select Training/Testing Approaches  \n",
    "    c) Train   \n",
    "  \n",
    "  \n",
    "**Model Evaluation**  \n",
    "In this stage, the models are evaluated with the help of specific performance metrics. With these metrics, we can go on to \n",
    "tune the hyperparameters of a model in order to improve it. This process is called hyperparameter optimization. We will \n",
    "repeat this step until we are satisfied with the performance.  \n",
    "  \n",
    "    4. Evaluate Algorithms  \n",
    "    a) Split-out validation dataset  \n",
    "    b) Test options and evaluation metric  \n",
    "    c) Spot Check Algorithms  \n",
    "    d) Compare Algorithms  \n",
    "  \n",
    "**Prediction**  \n",
    "Once we are happy with the results from the evaluation step, we will then move on to predictions. Predictions are made \n",
    "by the trained model when it is exposed to a new dataset. In a business setting, these predictions can be shared with \n",
    "decision makers to make effective business choices.  \n",
    "  \n",
    "    5. Improve Accuracy  \n",
    "    a) Algorithm Tuning  \n",
    "    b) Ensembles  \n",
    "\n",
    "**Model Deployment**  \n",
    "The whole process of machine learning does not just stop with model building and prediction. It also involves making use \n",
    "of the model to build an application with the new data. Depending on the business requirements, the deployment may be a \n",
    "report, or it may be some repetitive data science steps that are to be executed. After deployment, a model needs proper \n",
    "management and maintenance at regular intervals to keep it up and running.  \n",
    "\n",
    "    6. Finalize Model  \n",
    "    a) Predictions on validation dataset  \n",
    "    b) Create standalone model on entire training dataset  \n",
    "    c) Save model for later use  \n",
    "\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"I\"></a>\n",
    "\n",
    "# I.  Reading Data - Exploratory Data Analysis with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article outline\n",
    "1. Demonstration of main Pandas methods\n",
    "2. First attempt on predicting Auto Insurance Fraud\n",
    "3. Useful resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Demonstration of main Pandas methods \n",
    "\n",
    "**[Pandas](http://pandas.pydata.org)** is a Python library that provides extensive means for data analysis. Data scientists often work with data stored in table formats like `.csv`, `.tsv`, or `.xlsx`. Pandas makes it very convenient to load, process, and analyze such tabular data using SQL-like queries. In conjunction with `Matplotlib` and `Seaborn`, `Pandas` provides a wide range of opportunities for visual analysis of tabular data.\n",
    "\n",
    "The main data structures in `Pandas` are implemented with **Series** and **DataFrame** classes. The former is a one-dimensional indexed array of some fixed data type. The latter is a two-dimensional data structure - a table - where each column contains data of the same type. You can see it as a dictionary of `Series` instances. `DataFrames` are great for representing real data: rows correspond to instances (examples, observations, etc.), and columns correspond to features of these instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()  #  Will import Seaborn functionalities\n",
    "\n",
    "\n",
    "# we don't like warnings\n",
    "# you can comment the following 2 lines if you'd like to\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We’ll demonstrate the main methods in action by analyzing a [dataset](https://bigml.com/user/francisco/gallery/dataset/5163ad540c0b5e5b22000383) on the churn rate of telecom operator clients. Let’s read the data (using `read_csv`), and take a look at the first 5 lines using the `head` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Disply all Columns\n",
    "pd.options.display.max_columns = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>months_as_customer</th>\n",
       "      <th>age</th>\n",
       "      <th>policy_number</th>\n",
       "      <th>policy_bind_date</th>\n",
       "      <th>policy_state</th>\n",
       "      <th>policy_csl</th>\n",
       "      <th>policy_deductable</th>\n",
       "      <th>policy_annual_premium</th>\n",
       "      <th>umbrella_limit</th>\n",
       "      <th>insured_zip</th>\n",
       "      <th>insured_sex</th>\n",
       "      <th>insured_education_level</th>\n",
       "      <th>insured_occupation</th>\n",
       "      <th>insured_hobbies</th>\n",
       "      <th>insured_relationship</th>\n",
       "      <th>capital-gains</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>incident_date</th>\n",
       "      <th>incident_type</th>\n",
       "      <th>collision_type</th>\n",
       "      <th>incident_severity</th>\n",
       "      <th>authorities_contacted</th>\n",
       "      <th>incident_state</th>\n",
       "      <th>incident_city</th>\n",
       "      <th>incident_location</th>\n",
       "      <th>incident_hour_of_the_day</th>\n",
       "      <th>number_of_vehicles_involved</th>\n",
       "      <th>property_damage</th>\n",
       "      <th>bodily_injuries</th>\n",
       "      <th>witnesses</th>\n",
       "      <th>police_report_available</th>\n",
       "      <th>total_claim_amount</th>\n",
       "      <th>injury_claim</th>\n",
       "      <th>property_claim</th>\n",
       "      <th>vehicle_claim</th>\n",
       "      <th>auto_make</th>\n",
       "      <th>auto_model</th>\n",
       "      <th>auto_year</th>\n",
       "      <th>fraud_reported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>328</td>\n",
       "      <td>48</td>\n",
       "      <td>521585</td>\n",
       "      <td>10/17/2014</td>\n",
       "      <td>OH</td>\n",
       "      <td>250/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1406.91</td>\n",
       "      <td>0</td>\n",
       "      <td>466132</td>\n",
       "      <td>MALE</td>\n",
       "      <td>MD</td>\n",
       "      <td>craft-repair</td>\n",
       "      <td>sleeping</td>\n",
       "      <td>husband</td>\n",
       "      <td>53300</td>\n",
       "      <td>0</td>\n",
       "      <td>1/25/2015</td>\n",
       "      <td>Single Vehicle Collision</td>\n",
       "      <td>Side Collision</td>\n",
       "      <td>Major Damage</td>\n",
       "      <td>Police</td>\n",
       "      <td>SC</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>9935 4th Drive</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>YES</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>YES</td>\n",
       "      <td>71610</td>\n",
       "      <td>6510</td>\n",
       "      <td>13020</td>\n",
       "      <td>52080</td>\n",
       "      <td>Saab</td>\n",
       "      <td>92x</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>228</td>\n",
       "      <td>42</td>\n",
       "      <td>342868</td>\n",
       "      <td>6/27/2006</td>\n",
       "      <td>IN</td>\n",
       "      <td>250/500</td>\n",
       "      <td>2000</td>\n",
       "      <td>1197.22</td>\n",
       "      <td>5000000</td>\n",
       "      <td>468176</td>\n",
       "      <td>MALE</td>\n",
       "      <td>MD</td>\n",
       "      <td>machine-op-inspct</td>\n",
       "      <td>reading</td>\n",
       "      <td>other-relative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1/21/2015</td>\n",
       "      <td>Vehicle Theft</td>\n",
       "      <td>?</td>\n",
       "      <td>Minor Damage</td>\n",
       "      <td>Police</td>\n",
       "      <td>VA</td>\n",
       "      <td>Riverwood</td>\n",
       "      <td>6608 MLK Hwy</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>5070</td>\n",
       "      <td>780</td>\n",
       "      <td>780</td>\n",
       "      <td>3510</td>\n",
       "      <td>Mercedes</td>\n",
       "      <td>E400</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>134</td>\n",
       "      <td>29</td>\n",
       "      <td>687698</td>\n",
       "      <td>9/6/2000</td>\n",
       "      <td>OH</td>\n",
       "      <td>100/300</td>\n",
       "      <td>2000</td>\n",
       "      <td>1413.14</td>\n",
       "      <td>5000000</td>\n",
       "      <td>430632</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>PhD</td>\n",
       "      <td>sales</td>\n",
       "      <td>board-games</td>\n",
       "      <td>own-child</td>\n",
       "      <td>35100</td>\n",
       "      <td>0</td>\n",
       "      <td>2/22/2015</td>\n",
       "      <td>Multi-vehicle Collision</td>\n",
       "      <td>Rear Collision</td>\n",
       "      <td>Minor Damage</td>\n",
       "      <td>Police</td>\n",
       "      <td>NY</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>7121 Francis Lane</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>NO</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NO</td>\n",
       "      <td>34650</td>\n",
       "      <td>7700</td>\n",
       "      <td>3850</td>\n",
       "      <td>23100</td>\n",
       "      <td>Dodge</td>\n",
       "      <td>RAM</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256</td>\n",
       "      <td>41</td>\n",
       "      <td>227811</td>\n",
       "      <td>5/25/1990</td>\n",
       "      <td>IL</td>\n",
       "      <td>250/500</td>\n",
       "      <td>2000</td>\n",
       "      <td>1415.74</td>\n",
       "      <td>6000000</td>\n",
       "      <td>608117</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>PhD</td>\n",
       "      <td>armed-forces</td>\n",
       "      <td>board-games</td>\n",
       "      <td>unmarried</td>\n",
       "      <td>48900</td>\n",
       "      <td>-62400</td>\n",
       "      <td>1/10/2015</td>\n",
       "      <td>Single Vehicle Collision</td>\n",
       "      <td>Front Collision</td>\n",
       "      <td>Major Damage</td>\n",
       "      <td>Police</td>\n",
       "      <td>OH</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>6956 Maple Drive</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NO</td>\n",
       "      <td>63400</td>\n",
       "      <td>6340</td>\n",
       "      <td>6340</td>\n",
       "      <td>50720</td>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>Tahoe</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>228</td>\n",
       "      <td>44</td>\n",
       "      <td>367455</td>\n",
       "      <td>6/6/2014</td>\n",
       "      <td>IL</td>\n",
       "      <td>500/1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1583.91</td>\n",
       "      <td>6000000</td>\n",
       "      <td>610706</td>\n",
       "      <td>MALE</td>\n",
       "      <td>Associate</td>\n",
       "      <td>sales</td>\n",
       "      <td>board-games</td>\n",
       "      <td>unmarried</td>\n",
       "      <td>66000</td>\n",
       "      <td>-46000</td>\n",
       "      <td>2/17/2015</td>\n",
       "      <td>Vehicle Theft</td>\n",
       "      <td>?</td>\n",
       "      <td>Minor Damage</td>\n",
       "      <td>None</td>\n",
       "      <td>NY</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>3041 3rd Ave</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "      <td>6500</td>\n",
       "      <td>1300</td>\n",
       "      <td>650</td>\n",
       "      <td>4550</td>\n",
       "      <td>Accura</td>\n",
       "      <td>RSX</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>941851</td>\n",
       "      <td>7/16/1991</td>\n",
       "      <td>OH</td>\n",
       "      <td>500/1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1310.80</td>\n",
       "      <td>0</td>\n",
       "      <td>431289</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>Masters</td>\n",
       "      <td>craft-repair</td>\n",
       "      <td>paintball</td>\n",
       "      <td>unmarried</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2/22/2015</td>\n",
       "      <td>Single Vehicle Collision</td>\n",
       "      <td>Front Collision</td>\n",
       "      <td>Minor Damage</td>\n",
       "      <td>Fire</td>\n",
       "      <td>NC</td>\n",
       "      <td>Northbrook</td>\n",
       "      <td>6045 Andromedia St</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>87200</td>\n",
       "      <td>17440</td>\n",
       "      <td>8720</td>\n",
       "      <td>61040</td>\n",
       "      <td>Honda</td>\n",
       "      <td>Accord</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>285</td>\n",
       "      <td>41</td>\n",
       "      <td>186934</td>\n",
       "      <td>1/5/2014</td>\n",
       "      <td>IL</td>\n",
       "      <td>100/300</td>\n",
       "      <td>1000</td>\n",
       "      <td>1436.79</td>\n",
       "      <td>0</td>\n",
       "      <td>608177</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>PhD</td>\n",
       "      <td>prof-specialty</td>\n",
       "      <td>sleeping</td>\n",
       "      <td>wife</td>\n",
       "      <td>70900</td>\n",
       "      <td>0</td>\n",
       "      <td>1/24/2015</td>\n",
       "      <td>Single Vehicle Collision</td>\n",
       "      <td>Rear Collision</td>\n",
       "      <td>Major Damage</td>\n",
       "      <td>Fire</td>\n",
       "      <td>SC</td>\n",
       "      <td>Northbend</td>\n",
       "      <td>3092 Texas Drive</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>YES</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>?</td>\n",
       "      <td>108480</td>\n",
       "      <td>18080</td>\n",
       "      <td>18080</td>\n",
       "      <td>72320</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>Passat</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>130</td>\n",
       "      <td>34</td>\n",
       "      <td>918516</td>\n",
       "      <td>2/17/2003</td>\n",
       "      <td>OH</td>\n",
       "      <td>250/500</td>\n",
       "      <td>500</td>\n",
       "      <td>1383.49</td>\n",
       "      <td>3000000</td>\n",
       "      <td>442797</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>Masters</td>\n",
       "      <td>armed-forces</td>\n",
       "      <td>bungie-jumping</td>\n",
       "      <td>other-relative</td>\n",
       "      <td>35100</td>\n",
       "      <td>0</td>\n",
       "      <td>1/23/2015</td>\n",
       "      <td>Multi-vehicle Collision</td>\n",
       "      <td>Side Collision</td>\n",
       "      <td>Minor Damage</td>\n",
       "      <td>Police</td>\n",
       "      <td>NC</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>7629 5th St</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>YES</td>\n",
       "      <td>67500</td>\n",
       "      <td>7500</td>\n",
       "      <td>7500</td>\n",
       "      <td>52500</td>\n",
       "      <td>Suburu</td>\n",
       "      <td>Impreza</td>\n",
       "      <td>1996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>458</td>\n",
       "      <td>62</td>\n",
       "      <td>533940</td>\n",
       "      <td>11/18/2011</td>\n",
       "      <td>IL</td>\n",
       "      <td>500/1000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1356.92</td>\n",
       "      <td>5000000</td>\n",
       "      <td>441714</td>\n",
       "      <td>MALE</td>\n",
       "      <td>Associate</td>\n",
       "      <td>handlers-cleaners</td>\n",
       "      <td>base-jumping</td>\n",
       "      <td>wife</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2/26/2015</td>\n",
       "      <td>Single Vehicle Collision</td>\n",
       "      <td>Rear Collision</td>\n",
       "      <td>Major Damage</td>\n",
       "      <td>Other</td>\n",
       "      <td>NY</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>6128 Elm Lane</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>YES</td>\n",
       "      <td>46980</td>\n",
       "      <td>5220</td>\n",
       "      <td>5220</td>\n",
       "      <td>36540</td>\n",
       "      <td>Audi</td>\n",
       "      <td>A5</td>\n",
       "      <td>1998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>456</td>\n",
       "      <td>60</td>\n",
       "      <td>556080</td>\n",
       "      <td>11/11/1996</td>\n",
       "      <td>OH</td>\n",
       "      <td>250/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>766.19</td>\n",
       "      <td>0</td>\n",
       "      <td>612260</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>Associate</td>\n",
       "      <td>sales</td>\n",
       "      <td>kayaking</td>\n",
       "      <td>husband</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2/26/2015</td>\n",
       "      <td>Parked Car</td>\n",
       "      <td>?</td>\n",
       "      <td>Minor Damage</td>\n",
       "      <td>Police</td>\n",
       "      <td>WV</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>1416 Cherokee Ridge</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>?</td>\n",
       "      <td>5060</td>\n",
       "      <td>460</td>\n",
       "      <td>920</td>\n",
       "      <td>3680</td>\n",
       "      <td>Mercedes</td>\n",
       "      <td>E400</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     months_as_customer  age  policy_number policy_bind_date policy_state  \\\n",
       "0                   328   48         521585       10/17/2014           OH   \n",
       "1                   228   42         342868        6/27/2006           IN   \n",
       "2                   134   29         687698         9/6/2000           OH   \n",
       "3                   256   41         227811        5/25/1990           IL   \n",
       "4                   228   44         367455         6/6/2014           IL   \n",
       "..                  ...  ...            ...              ...          ...   \n",
       "995                   3   38         941851        7/16/1991           OH   \n",
       "996                 285   41         186934         1/5/2014           IL   \n",
       "997                 130   34         918516        2/17/2003           OH   \n",
       "998                 458   62         533940       11/18/2011           IL   \n",
       "999                 456   60         556080       11/11/1996           OH   \n",
       "\n",
       "    policy_csl  policy_deductable  policy_annual_premium  umbrella_limit  \\\n",
       "0      250/500               1000                1406.91               0   \n",
       "1      250/500               2000                1197.22         5000000   \n",
       "2      100/300               2000                1413.14         5000000   \n",
       "3      250/500               2000                1415.74         6000000   \n",
       "4     500/1000               1000                1583.91         6000000   \n",
       "..         ...                ...                    ...             ...   \n",
       "995   500/1000               1000                1310.80               0   \n",
       "996    100/300               1000                1436.79               0   \n",
       "997    250/500                500                1383.49         3000000   \n",
       "998   500/1000               2000                1356.92         5000000   \n",
       "999    250/500               1000                 766.19               0   \n",
       "\n",
       "     insured_zip insured_sex insured_education_level insured_occupation  \\\n",
       "0         466132        MALE                      MD       craft-repair   \n",
       "1         468176        MALE                      MD  machine-op-inspct   \n",
       "2         430632      FEMALE                     PhD              sales   \n",
       "3         608117      FEMALE                     PhD       armed-forces   \n",
       "4         610706        MALE               Associate              sales   \n",
       "..           ...         ...                     ...                ...   \n",
       "995       431289      FEMALE                 Masters       craft-repair   \n",
       "996       608177      FEMALE                     PhD     prof-specialty   \n",
       "997       442797      FEMALE                 Masters       armed-forces   \n",
       "998       441714        MALE               Associate  handlers-cleaners   \n",
       "999       612260      FEMALE               Associate              sales   \n",
       "\n",
       "    insured_hobbies insured_relationship  capital-gains  capital-loss  \\\n",
       "0          sleeping              husband          53300             0   \n",
       "1           reading       other-relative              0             0   \n",
       "2       board-games            own-child          35100             0   \n",
       "3       board-games            unmarried          48900        -62400   \n",
       "4       board-games            unmarried          66000        -46000   \n",
       "..              ...                  ...            ...           ...   \n",
       "995       paintball            unmarried              0             0   \n",
       "996        sleeping                 wife          70900             0   \n",
       "997  bungie-jumping       other-relative          35100             0   \n",
       "998    base-jumping                 wife              0             0   \n",
       "999        kayaking              husband              0             0   \n",
       "\n",
       "    incident_date             incident_type   collision_type  \\\n",
       "0       1/25/2015  Single Vehicle Collision   Side Collision   \n",
       "1       1/21/2015             Vehicle Theft                ?   \n",
       "2       2/22/2015   Multi-vehicle Collision   Rear Collision   \n",
       "3       1/10/2015  Single Vehicle Collision  Front Collision   \n",
       "4       2/17/2015             Vehicle Theft                ?   \n",
       "..            ...                       ...              ...   \n",
       "995     2/22/2015  Single Vehicle Collision  Front Collision   \n",
       "996     1/24/2015  Single Vehicle Collision   Rear Collision   \n",
       "997     1/23/2015   Multi-vehicle Collision   Side Collision   \n",
       "998     2/26/2015  Single Vehicle Collision   Rear Collision   \n",
       "999     2/26/2015                Parked Car                ?   \n",
       "\n",
       "    incident_severity authorities_contacted incident_state incident_city  \\\n",
       "0        Major Damage                Police             SC      Columbus   \n",
       "1        Minor Damage                Police             VA     Riverwood   \n",
       "2        Minor Damage                Police             NY      Columbus   \n",
       "3        Major Damage                Police             OH     Arlington   \n",
       "4        Minor Damage                  None             NY     Arlington   \n",
       "..                ...                   ...            ...           ...   \n",
       "995      Minor Damage                  Fire             NC    Northbrook   \n",
       "996      Major Damage                  Fire             SC     Northbend   \n",
       "997      Minor Damage                Police             NC     Arlington   \n",
       "998      Major Damage                 Other             NY     Arlington   \n",
       "999      Minor Damage                Police             WV      Columbus   \n",
       "\n",
       "       incident_location  incident_hour_of_the_day  \\\n",
       "0         9935 4th Drive                         5   \n",
       "1           6608 MLK Hwy                         8   \n",
       "2      7121 Francis Lane                         7   \n",
       "3       6956 Maple Drive                         5   \n",
       "4           3041 3rd Ave                        20   \n",
       "..                   ...                       ...   \n",
       "995   6045 Andromedia St                        20   \n",
       "996     3092 Texas Drive                        23   \n",
       "997          7629 5th St                         4   \n",
       "998        6128 Elm Lane                         2   \n",
       "999  1416 Cherokee Ridge                         6   \n",
       "\n",
       "     number_of_vehicles_involved property_damage  bodily_injuries  witnesses  \\\n",
       "0                              1             YES                1          2   \n",
       "1                              1               ?                0          0   \n",
       "2                              3              NO                2          3   \n",
       "3                              1               ?                1          2   \n",
       "4                              1              NO                0          1   \n",
       "..                           ...             ...              ...        ...   \n",
       "995                            1             YES                0          1   \n",
       "996                            1             YES                2          3   \n",
       "997                            3               ?                2          3   \n",
       "998                            1               ?                0          1   \n",
       "999                            1               ?                0          3   \n",
       "\n",
       "    police_report_available  total_claim_amount  injury_claim  property_claim  \\\n",
       "0                       YES               71610          6510           13020   \n",
       "1                         ?                5070           780             780   \n",
       "2                        NO               34650          7700            3850   \n",
       "3                        NO               63400          6340            6340   \n",
       "4                        NO                6500          1300             650   \n",
       "..                      ...                 ...           ...             ...   \n",
       "995                       ?               87200         17440            8720   \n",
       "996                       ?              108480         18080           18080   \n",
       "997                     YES               67500          7500            7500   \n",
       "998                     YES               46980          5220            5220   \n",
       "999                       ?                5060           460             920   \n",
       "\n",
       "     vehicle_claim   auto_make auto_model  auto_year  fraud_reported  \n",
       "0            52080        Saab        92x       2004               1  \n",
       "1             3510    Mercedes       E400       2007               1  \n",
       "2            23100       Dodge        RAM       2007               0  \n",
       "3            50720   Chevrolet      Tahoe       2014               1  \n",
       "4             4550      Accura        RSX       2009               0  \n",
       "..             ...         ...        ...        ...             ...  \n",
       "995          61040       Honda     Accord       2006               0  \n",
       "996          72320  Volkswagen     Passat       2015               0  \n",
       "997          52500      Suburu    Impreza       1996               0  \n",
       "998          36540        Audi         A5       1998               0  \n",
       "999           3680    Mercedes       E400       2007               0  \n",
       "\n",
       "[1000 rows x 39 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autinsurance = pd.read_csv('insurance_claims.csv')\n",
    "autinsurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print () function\n",
    "# Name of the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autinsurance.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autinsurance.isna().sum()  ## Count Filled cells\n",
    "                             ## Sum empty cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>About printing DataFrames in Jupyter notebooks</summary>\n",
    "<p>\n",
    "In Jupyter notebooks, Pandas DataFrames are printed as these pretty tables seen above while `print(df.head())` looks worse.\n",
    "By default, Pandas displays 20 columns and 60 rows, so, if your DataFrame is bigger, use the `set_option` function as shown in the example below:\n",
    "\n",
    "```python\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "```\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "Recall that each row corresponds to one client, an **instance**, and columns are **features** of this instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rudimentary Cleaning\n",
    "\n",
    "The cell below will load our dataset in, and perform some rudimentary cleaning.  \n",
    "\n",
    "Do not worry if you do not understand all of the code below. Comments are provided if you are interested in following along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Column Names:\n",
      " Index(['months_as_customer', 'age', 'policy_number', 'policy_bind_date',\n",
      "       'policy_state', 'policy_csl', 'policy_deductable',\n",
      "       'policy_annual_premium', 'umbrella_limit', 'insured_zip', 'insured_sex',\n",
      "       'insured_education_level', 'insured_occupation', 'insured_hobbies',\n",
      "       'insured_relationship', 'capital-gains', 'capital-loss',\n",
      "       'incident_date', 'incident_type', 'collision_type', 'incident_severity',\n",
      "       'authorities_contacted', 'incident_state', 'incident_city',\n",
      "       'incident_location', 'incident_hour_of_the_day',\n",
      "       'number_of_vehicles_involved', 'property_damage', 'bodily_injuries',\n",
      "       'witnesses', 'police_report_available', 'total_claim_amount',\n",
      "       'injury_claim', 'property_claim', 'vehicle_claim', 'auto_make',\n",
      "       'auto_model', 'auto_year', 'fraud_reported'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Column names may be accessed (and changed) using the `.columns` attribute as below\n",
    "print(\"Old Column Names:\\n\", autinsurance.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Janitor Column Names:\n",
      " Index(['months_as_customer', 'age', 'policy_number', 'policy_bind_date',\n",
      "       'policy_state', 'policy_csl', 'policy_deductable',\n",
      "       'policy_annual_premium', 'umbrella_limit', 'insured_zip', 'insured_sex',\n",
      "       'insured_education_level', 'insured_occupation', 'insured_hobbies',\n",
      "       'insured_relationship', 'capital_gains', 'capital_loss',\n",
      "       'incident_date', 'incident_type', 'collision_type', 'incident_severity',\n",
      "       'authorities_contacted', 'incident_state', 'incident_city',\n",
      "       'incident_location', 'incident_hour_of_the_day',\n",
      "       'number_of_vehicles_involved', 'property_damage', 'bodily_injuries',\n",
      "       'witnesses', 'police_report_available', 'total_claim_amount',\n",
      "       'injury_claim', 'property_claim', 'vehicle_claim', 'auto_make',\n",
      "       'auto_model', 'auto_year', 'fraud_reported'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#! pip install pyjanitor \n",
    "import janitor\n",
    "autinsurance = autinsurance.clean_names()\n",
    "print(\"New Janitor Column Names:\\n\", autinsurance.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stripping out spaces from ends of names, and replacing internal spaces with \"_\"\n",
    "# print(\"\\nStripping spaces from ends of column names; replacing internal spaces with '_'\\n\")\n",
    "# autinsurance.columns = [col.strip().replace(' ', '_').lower() for col in autinsurance.columns]\n",
    "# autinsurance.columns = [col.strip().replace('-', '_').lower() for col in autinsurance.columns]\n",
    "# # Print edited column names\n",
    "# print(\"\\nNew Column Names:\\n\", autinsurance.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Few Useful Functions\n",
    "\n",
    "With data in hand, it is time to look at a few functions which will be critical to your succes in understanding and completeing this assignment. All of the functions come from the \"`numpy`\" package. \"`Numpy`\" will be more completely introduced in week 8.  \n",
    "\n",
    "The functions are:  \n",
    "\n",
    "- `np.mean` - For calculating a mean\n",
    "- `np.std` - For calculating standard deviation\n",
    "- `np.var` - For calculating variance\n",
    "- `np.ptp` - For calculating range (ptp stands for \"point to point\")\n",
    "- `np.sqrt` - For taking the square root of a number (instead of x \\*\\* .5)\n",
    "- `np.min` - For finding the minimum value of a collection\n",
    "- `np.max` - For finding the maximum value of a collection \n",
    "\n",
    "\n",
    "Of note, these functions work with most any type of collection: list, tuple, array, etc; through not with dictionaries.  \n",
    "\n",
    "In the below examples, the \"sales\" and \"transactions\" data are accessed via \"`office['sales']`\" and \"`office['transactions']`\" respectively. Accessing the data this way yeilds a \"`Pandas Series`\"; a data type that will be covered more completely in week 9. For the purposes here, the `Series` may be thought of as a special type of list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s have a look at data dimensionality, features names, and feature types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(autinsurance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 39)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autinsurance.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can see that the table contains 3333 rows and 20 columns.\n",
    "\n",
    "Now let’s try printing out column names using `columns`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(autinsurance.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `info()` method to output some general information about the dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 39 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   months_as_customer           1000 non-null   int64  \n",
      " 1   age                          1000 non-null   int64  \n",
      " 2   policy_number                1000 non-null   int64  \n",
      " 3   policy_bind_date             1000 non-null   object \n",
      " 4   policy_state                 1000 non-null   object \n",
      " 5   policy_csl                   1000 non-null   object \n",
      " 6   policy_deductable            1000 non-null   int64  \n",
      " 7   policy_annual_premium        1000 non-null   float64\n",
      " 8   umbrella_limit               1000 non-null   int64  \n",
      " 9   insured_zip                  1000 non-null   int64  \n",
      " 10  insured_sex                  1000 non-null   object \n",
      " 11  insured_education_level      1000 non-null   object \n",
      " 12  insured_occupation           1000 non-null   object \n",
      " 13  insured_hobbies              1000 non-null   object \n",
      " 14  insured_relationship         1000 non-null   object \n",
      " 15  capital_gains                1000 non-null   int64  \n",
      " 16  capital_loss                 1000 non-null   int64  \n",
      " 17  incident_date                1000 non-null   object \n",
      " 18  incident_type                1000 non-null   object \n",
      " 19  collision_type               1000 non-null   object \n",
      " 20  incident_severity            1000 non-null   object \n",
      " 21  authorities_contacted        1000 non-null   object \n",
      " 22  incident_state               1000 non-null   object \n",
      " 23  incident_city                1000 non-null   object \n",
      " 24  incident_location            1000 non-null   object \n",
      " 25  incident_hour_of_the_day     1000 non-null   int64  \n",
      " 26  number_of_vehicles_involved  1000 non-null   int64  \n",
      " 27  property_damage              1000 non-null   object \n",
      " 28  bodily_injuries              1000 non-null   int64  \n",
      " 29  witnesses                    1000 non-null   int64  \n",
      " 30  police_report_available      1000 non-null   object \n",
      " 31  total_claim_amount           1000 non-null   int64  \n",
      " 32  injury_claim                 1000 non-null   int64  \n",
      " 33  property_claim               1000 non-null   int64  \n",
      " 34  vehicle_claim                1000 non-null   int64  \n",
      " 35  auto_make                    1000 non-null   object \n",
      " 36  auto_model                   1000 non-null   object \n",
      " 37  auto_year                    1000 non-null   int64  \n",
      " 38  fraud_reported               1000 non-null   int64  \n",
      "dtypes: float64(1), int64(18), object(20)\n",
      "memory usage: 304.8+ KB\n"
     ]
    }
   ],
   "source": [
    "autinsurance.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bool`, `int64`, `float64` and `object` are the data types of our features. We see that one feature is logical (`bool`), 3 features are of type `object`, and 16 features are numeric. With this same method, we can easily see if there are any missing values. Here, there are none because each column contains 3333 observations, the same number of rows we saw before with `shape`.\n",
    "\n",
    "We can **change the column type** with the `astype` method. Let’s apply this method to the `Churn` feature to convert it into `int64`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autinsurance['fraud_reported'] = autinsurance['fraud_reported'].astype('int64') # Assuming your TV is of string type (Yes/No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows\n",
    "len(autinsurance)  # for traceability 576 rows imported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `describe` method shows basic statistical characteristics of each numerical feature (`int64` and `float64` types): number of non-missing values, mean, standard deviation, range, median, 0.25 and 0.75 quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>months_as_customer</th>\n",
       "      <th>age</th>\n",
       "      <th>policy_number</th>\n",
       "      <th>policy_deductable</th>\n",
       "      <th>policy_annual_premium</th>\n",
       "      <th>umbrella_limit</th>\n",
       "      <th>insured_zip</th>\n",
       "      <th>capital_gains</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>incident_hour_of_the_day</th>\n",
       "      <th>number_of_vehicles_involved</th>\n",
       "      <th>bodily_injuries</th>\n",
       "      <th>witnesses</th>\n",
       "      <th>total_claim_amount</th>\n",
       "      <th>injury_claim</th>\n",
       "      <th>property_claim</th>\n",
       "      <th>vehicle_claim</th>\n",
       "      <th>auto_year</th>\n",
       "      <th>fraud_reported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>203.954000</td>\n",
       "      <td>38.948000</td>\n",
       "      <td>546238.648000</td>\n",
       "      <td>1136.000000</td>\n",
       "      <td>1256.406150</td>\n",
       "      <td>1.101000e+06</td>\n",
       "      <td>501214.488000</td>\n",
       "      <td>25126.100000</td>\n",
       "      <td>-26793.700000</td>\n",
       "      <td>11.644000</td>\n",
       "      <td>1.83900</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>1.487000</td>\n",
       "      <td>52761.94000</td>\n",
       "      <td>7433.420000</td>\n",
       "      <td>7399.570000</td>\n",
       "      <td>37928.950000</td>\n",
       "      <td>2005.103000</td>\n",
       "      <td>0.247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>115.113174</td>\n",
       "      <td>9.140287</td>\n",
       "      <td>257063.005276</td>\n",
       "      <td>611.864673</td>\n",
       "      <td>244.167395</td>\n",
       "      <td>2.297407e+06</td>\n",
       "      <td>71701.610941</td>\n",
       "      <td>27872.187708</td>\n",
       "      <td>28104.096686</td>\n",
       "      <td>6.951373</td>\n",
       "      <td>1.01888</td>\n",
       "      <td>0.820127</td>\n",
       "      <td>1.111335</td>\n",
       "      <td>26401.53319</td>\n",
       "      <td>4880.951853</td>\n",
       "      <td>4824.726179</td>\n",
       "      <td>18886.252893</td>\n",
       "      <td>6.015861</td>\n",
       "      <td>0.431483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>100804.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>433.330000</td>\n",
       "      <td>-1.000000e+06</td>\n",
       "      <td>430104.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-111100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1995.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>115.750000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>335980.250000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>1089.607500</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>448404.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-51500.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>41812.50000</td>\n",
       "      <td>4295.000000</td>\n",
       "      <td>4445.000000</td>\n",
       "      <td>30292.500000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>199.500000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>533135.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1257.200000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>466445.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-23250.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>58055.00000</td>\n",
       "      <td>6775.000000</td>\n",
       "      <td>6750.000000</td>\n",
       "      <td>42100.000000</td>\n",
       "      <td>2005.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>276.250000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>759099.750000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>1415.695000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>603251.000000</td>\n",
       "      <td>51025.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>70592.50000</td>\n",
       "      <td>11305.000000</td>\n",
       "      <td>10885.000000</td>\n",
       "      <td>50822.500000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>479.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>999435.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2047.590000</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>620962.000000</td>\n",
       "      <td>100500.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>114920.00000</td>\n",
       "      <td>21450.000000</td>\n",
       "      <td>23670.000000</td>\n",
       "      <td>79560.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       months_as_customer          age  policy_number  policy_deductable  \\\n",
       "count         1000.000000  1000.000000    1000.000000        1000.000000   \n",
       "mean           203.954000    38.948000  546238.648000        1136.000000   \n",
       "std            115.113174     9.140287  257063.005276         611.864673   \n",
       "min              0.000000    19.000000  100804.000000         500.000000   \n",
       "25%            115.750000    32.000000  335980.250000         500.000000   \n",
       "50%            199.500000    38.000000  533135.000000        1000.000000   \n",
       "75%            276.250000    44.000000  759099.750000        2000.000000   \n",
       "max            479.000000    64.000000  999435.000000        2000.000000   \n",
       "\n",
       "       policy_annual_premium  umbrella_limit    insured_zip  capital_gains  \\\n",
       "count            1000.000000    1.000000e+03    1000.000000    1000.000000   \n",
       "mean             1256.406150    1.101000e+06  501214.488000   25126.100000   \n",
       "std               244.167395    2.297407e+06   71701.610941   27872.187708   \n",
       "min               433.330000   -1.000000e+06  430104.000000       0.000000   \n",
       "25%              1089.607500    0.000000e+00  448404.500000       0.000000   \n",
       "50%              1257.200000    0.000000e+00  466445.500000       0.000000   \n",
       "75%              1415.695000    0.000000e+00  603251.000000   51025.000000   \n",
       "max              2047.590000    1.000000e+07  620962.000000  100500.000000   \n",
       "\n",
       "        capital_loss  incident_hour_of_the_day  number_of_vehicles_involved  \\\n",
       "count    1000.000000               1000.000000                   1000.00000   \n",
       "mean   -26793.700000                 11.644000                      1.83900   \n",
       "std     28104.096686                  6.951373                      1.01888   \n",
       "min   -111100.000000                  0.000000                      1.00000   \n",
       "25%    -51500.000000                  6.000000                      1.00000   \n",
       "50%    -23250.000000                 12.000000                      1.00000   \n",
       "75%         0.000000                 17.000000                      3.00000   \n",
       "max         0.000000                 23.000000                      4.00000   \n",
       "\n",
       "       bodily_injuries    witnesses  total_claim_amount  injury_claim  \\\n",
       "count      1000.000000  1000.000000          1000.00000   1000.000000   \n",
       "mean          0.992000     1.487000         52761.94000   7433.420000   \n",
       "std           0.820127     1.111335         26401.53319   4880.951853   \n",
       "min           0.000000     0.000000           100.00000      0.000000   \n",
       "25%           0.000000     1.000000         41812.50000   4295.000000   \n",
       "50%           1.000000     1.000000         58055.00000   6775.000000   \n",
       "75%           2.000000     2.000000         70592.50000  11305.000000   \n",
       "max           2.000000     3.000000        114920.00000  21450.000000   \n",
       "\n",
       "       property_claim  vehicle_claim    auto_year  fraud_reported  \n",
       "count     1000.000000    1000.000000  1000.000000     1000.000000  \n",
       "mean      7399.570000   37928.950000  2005.103000        0.247000  \n",
       "std       4824.726179   18886.252893     6.015861        0.431483  \n",
       "min          0.000000      70.000000  1995.000000        0.000000  \n",
       "25%       4445.000000   30292.500000  2000.000000        0.000000  \n",
       "50%       6750.000000   42100.000000  2005.000000        0.000000  \n",
       "75%      10885.000000   50822.500000  2010.000000        0.000000  \n",
       "max      23670.000000   79560.000000  2015.000000        1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autinsurance.describe() # Similar to summary() in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autinsurance.describe().T # Similar to summary() in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autinsurance.describe().transpose()  # change the rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install skimpy\n",
    "from skimpy import skim\n",
    "\n",
    "skim(autinsurance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autinsurance.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rich import print\n",
    "from skimpy import clean_columns\n",
    "\n",
    "columns = autinsurance.columns\n",
    "messy_df = pd.DataFrame(columns=columns, index=[0], data=[range(len(columns))])\n",
    "print(\"Column names:\")\n",
    "print(list(messy_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by the 'label' and show descriptive stats\n",
    "autinsurance.groupby('age').agg(['count', 'mean','std','min','max','median']).T.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see statistics on non-numerical features, one has to explicitly indicate data types of interest in the `include` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autinsurance.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical (type `object`) and boolean (type `bool`) features we can use the `value_counts` method. Let’s have a look at the distribution of `Churn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autinsurance['fraud_reported'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "753 users out of 247 are *loyal*; their `Fraud` value is `0`. To calculate fractions, pass `normalize=True` to the `value_counts` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autinsurance['fraud_reported'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(autinsurance['fraud_reported'].value_counts().plot(\n",
    "        kind='bar',\n",
    "        figsize=(4, 2),\n",
    "        title='Distribution of Target Variable',\n",
    "    )\n",
    ");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable, `bad_loan`, is **unbalanced** - meaning the variable contains about 4x more \"good\" loan instances than \"bad\" loan instances. This can present a problem since the **positive class** we want to predict is the \"bad\" loan class (`1.0`).\n",
    "\n",
    "Because of this unbalanced data, we will make sure that both our training set and testing set **maintain this ratio** of good:bad loans. This is acheived by using the `stratify` argument in the `train_test_split()` function, which was imported from the `sklearn.model_selection` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.  Indexing and retrieving data\n",
    "\n",
    "DataFrame can be indexed in different ways. \n",
    "\n",
    "To get a single column, you can use a `DataFrame['Name']` construction. Let's use this to answer a question about that column alone: **what is the proportion of churned users in our dataframe?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autinsurance['fraud_reported'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "14.5% is actually quite bad for a company; such a churn rate can make the company go bankrupt.\n",
    "\n",
    "**Boolean indexing** with one column is also very convenient. The syntax is `df[P(df['Name'])]`, where `P` is some logical condition that is checked for each element of the `Name` column. The result of such indexing is the DataFrame consisting only of rows that satisfy the `P` condition on the `Name` column. \n",
    "\n",
    "Let’s use it to answer the question:\n",
    "\n",
    "**What are average values of numerical features for churned users?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autinsurance[autinsurance['fraud_reported'] == 1].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "DataFrames can be indexed by column name (label) or row name (index) or by the serial number of a row. The `loc` method is used for **indexing by name**, while `iloc()` is used for **indexing by number**.\n",
    "\n",
    "In the first case, we would say *\"give us the values of the rows with index from 0 to 5 (inclusive) and columns labeled from State to Area code (inclusive)\"*, and, in the second case, we would say *\"give us the values of the first five rows in the first three columns\"* (as in typical Python slice: the maximal value is not included).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autinsurance.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autinsurance.loc[0:5, 'age':'witnesses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autinsurance.iloc[0:5, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need the first or the last line of the data frame, we can use the `df[:1]` or `df[-1:]` construct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autinsurance[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Applying Functions to Cells, Columns and Rows\n",
    "\n",
    "**To apply functions to each column, use `apply()`:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autinsurance.apply(np.max) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"II\"></a>\n",
    "# II. Visual data analysis in Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the field of Machine Learning, *data visualization* is not just making fancy graphics for reports; it is used extensively in day-to-day work for all phases of a project.\n",
    "\n",
    "To start with, visual exploration of data is the first thing one tends to do when dealing with a new task. We do preliminary checks and analysis using graphics and tables to summarize the data and leave out the less important details. It is much more convenient for us, humans, to grasp the main points this way than by reading many lines of raw data. It is amazing how much insight can be gained from seemingly simple charts created with available visualization tools.\n",
    "\n",
    "Next, when we analyze the performance of a model or report results, we also often use charts and images. Sometimes, for interpreting a complex model, we need to project high-dimensional spaces onto more visually intelligible 2D or 3D figures.\n",
    "\n",
    "All in all, visualization is a relatively fast way to learn something new about your data. Thus, it is vital to learn its most useful techniques and make them part of your everyday ML toolbox.\n",
    "\n",
    "We are going to get hands-on experience with visual exploration of data using popular libraries such as `matplotlib` and `seaborn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article outline\n",
    "\n",
    "1. Dataset\n",
    "2. Univariate visualization\n",
    "    * 2.1 Quantitative features\n",
    "    * 2.2 Categorical and binary features\n",
    "3. Multivariate visualization\n",
    "    * 3.1 Quantitative–Quantitative\n",
    "    * 3.2 Quantitative–Categorical\n",
    "    * 3.3 Categorical–Categorical\n",
    "4. Whole dataset\n",
    "    * 4.1 Naive approach\n",
    "    * 4.2 Dimensionality reduction\n",
    "    * 4.2 t-SNE\n",
    "5. Useful resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Univariate visualization\n",
    "\n",
    "*Univariate* analysis looks at one feature at a time. When we analyze a feature independently, we are usually mostly interested in the *distribution of its values* and ignore other features in the dataset.\n",
    "\n",
    "Below, we will consider different statistical types of features and the corresponding tools for their individual visual analysis.\n",
    "\n",
    "#### 1.1 Quantitative features\n",
    "\n",
    "*Quantitative features* take on ordered numerical values. Those values can be *discrete*, like integers, or *continuous*, like real numbers, and usually express a count or a measurement.\n",
    "\n",
    "##### 1.1.1 Histograms and density plots\n",
    "\n",
    "The easiest way to take a look at the distribution of a numerical variable is to plot its *histogram* using the `DataFrame`'s method [`hist()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.hist.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['age', 'months_as_customer', 'capital_gains', 'capital_loss']\n",
    "\n",
    "autinsurance[features].hist(figsize=(6, 4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A histogram groups values into *bins* of equal value range. The shape of the histogram may contain clues about the underlying distribution type: Gaussian, exponential etc. You can also spot any skewness in its shape when the distribution is nearly regular but has some anomalies. Knowing the distribution of the feature values becomes important when you use Machine Learning methods that assume a particular type of it, most often Gaussian.\n",
    "\n",
    "In the above plot, we see that the variable *Total day minutes* is normally distributed, while *Total intl calls* is prominently skewed right (its tail is longer on the right).\n",
    "\n",
    "There is also another, often clearer, way to grasp the distribution: *density plots* or, more formally, *Kernel Density Plots*. They can be considered a [smoothed](https://en.wikipedia.org/wiki/Kernel_smoother) version of the histogram. Their main advantage over the latter is that they do not depend on the size of the bins. Let's create density plots for the same two variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autinsurance[features].plot(kind='density', subplots=True, layout=(2, 2), \n",
    "                  sharex=False, figsize=(6, 4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to plot a distribution of observations with `seaborn`'s [`distplot()`](https://seaborn.pydata.org/generated/seaborn.distplot.html). For example, let's look at the distribution of *Total day minutes*. By default, the plot displays both the histogram with the [kernel density estimate](https://en.wikipedia.org/wiki/Kernel_density_estimation) (KDE) on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increasing the width of the Chart\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = 5,5 # similar to par(mfrow = c(2,1), mar = c(4,4,2,1)) # 2 columns and 1 row\n",
    "sns.distplot(autinsurance[\"age\"]) # pass it one variable\n",
    "\n",
    "# if you are getting warnings related to the package you should use ignore function\n",
    "import warnings\n",
    "warnings.filterwarnings ('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# increasing the width of the Chart\n",
    "plt.rcParams['figure.figsize'] = 4,4 # similar to par(mfrow = c(2,1), mar = c(4,4,2,1)) # 2 columns and 1 row\n",
    "sns.distplot(autinsurance[\"months_as_customer\"]) # pass it one variable\n",
    "\n",
    "# if you are getting warnings related to the package you should use ignore function\n",
    "import warnings\n",
    "warnings.filterwarnings ('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Categorical and binary features\n",
    "\n",
    "*Categorical features* take on a fixed number of values. Each of these values assigns an observation to a corresponding group, known as a *category*, which reflects some qualitative property of this example. *Binary* variables are an important special case of categorical variables when the number of possible values is exactly 2. If the values of a categorical variable are ordered, it is called *ordinal*.\n",
    "\n",
    "##### 1.2.1 Frequency table\n",
    "\n",
    "Let’s check the class balance in our dataset by looking at the distribution of the target variable: the *churn rate*. First, we will get a frequency table, which shows how frequent each value of the categorical variable is. For this, we will use the [`value_counts()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autinsurance['fraud_reported'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the entries in the output are sorted from the most to the least frequently-occurring values.\n",
    "\n",
    "In our case, the data is not *balanced*; that is, our two target classes, loyal and disloyal customers, are not represented equally in the dataset. Only a small part of the clients canceled their subscription to the telecom service. As we will see in the following articles, this fact may imply some restrictions on measuring the classification performance, and, in the future, we may want to additionaly penalize our model errors in predicting the minority \"Churn\" class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2 Bar plot\n",
    "\n",
    "The bar plot is a graphical representation of the frequency table. The easiest way to create it is to use the `seaborn`'s function [`countplot()`](https://seaborn.pydata.org/generated/seaborn.countplot.html). There is another function in `seaborn` that is somewhat confusingly called [`barplot()`](https://seaborn.pydata.org/generated/seaborn.barplot.html) and is mostly used for representation of some basic statistics of a numerical variable grouped by a categorical feature.\n",
    "\n",
    "Let's plot the distributions for two categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
    "\n",
    "sns.countplot(x='policy_state', data=autinsurance, ax=axes[0]);\n",
    "sns.countplot(x='incident_state', data=autinsurance, ax=axes[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 7))\n",
    "\n",
    "sns.countplot(x='insured_education_level', data=autinsurance, ax=axes[0]);\n",
    "sns.countplot(x='insured_relationship', data=autinsurance, ax=axes[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3. Distributions of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 7))\n",
    "\n",
    "sns.countplot(x='insured_sex', data=autinsurance, ax=axes[0]);\n",
    "sns.countplot(x='incident_state', data=autinsurance, ax=axes[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributions of categorical features\n",
    "plt.rcParams['figure.figsize'] = 8,6\n",
    "sns.countplot(y='insured_sex', data=autinsurance)\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(y='incident_state', data=autinsurance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the histograms, discussed above, and bar plots may look similar, there are several differences between them:\n",
    "1. *Histograms* are best suited for looking at the distribution of numerical variables while *bar plots* are used for categorical features.\n",
    "2. The values on the X-axis in the *histogram* are numerical; a *bar plot* can have any type of values on the X-axis: numbers, strings, booleans.\n",
    "3. The *histogram*'s X-axis is a *Cartesian coordinate axis* along which values cannot be changed; the ordering of the *bars* is not predefined. Still, it is useful to note that the bars are often sorted by height, that is, the frequency of the values. Also, when we consider *ordinal* variables (like *Customer service calls* in our data), the bars are usually ordered by variable value.\n",
    "\n",
    "The left chart above vividly illustrates the imbalance in our target variable. The bar plot for *Customer service calls* on the right gives a hint that the majority of customers resolve their problems in maximum 2–3 calls. But, as we want to be able to predict the minority class, we may be more interested in how the fewer dissatisfied customers behave. It may well be that the tail of that bar plot contains most of our churn. These are just hypotheses for now, so let's move on to some more interesting and powerful visual techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multivariate visualization\n",
    "\n",
    "*Multivariate* plots allow us to see relationships between two and more different variables, all in one figure. Just as in the case of univariate plots, the specific type of visualization will depend on the types of the variables being analyzed.\n",
    "\n",
    "#### 2.1 Quantitative–Quantitative\n",
    "\n",
    "##### 2.1.1 Correlation matrix\n",
    "\n",
    "Let's look at the correlations among the numerical variables in our dataset. This information is important to know as there are Machine Learning algorithms (for example, linear and logistic regression) that do not handle highly correlated input variables well.\n",
    "\n",
    "First, we will use the method [`corr()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html) on a `DataFrame` that calculates the correlation between each pair of features. Then, we pass the resulting *correlation matrix* to [`heatmap()`](https://seaborn.pydata.org/generated/seaborn.heatmap.html) from `seaborn`, which renders a color-coded matrix for the provided values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autinsurance.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = autinsurance.corr(method = 'kendall')  # corr(autinsurance)\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Highly correlated items = not good!</h4>\n",
    "<h4>Low correlated items = good </h4>\n",
    "<h4>Correlations with target (dv) = good (high predictive power)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autinsurance = autinsurance.drop(\"morocco\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 4.2 Correlation heatmap of the numberic variables\n",
    "plt.rcParams['figure.figsize'] = 10,6  # control plot sizeimport seaborn as sns\n",
    "sns.heatmap(autinsurance.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn\n",
    "## first_twenty = har_train.iloc[:, :20] # pull out first 20 feats\n",
    "corr = autinsurance.corr()  # compute correlation matrix\n",
    "mask = np.zeros_like(corr, dtype=np.bool)  # make mask\n",
    "mask[np.triu_indices_from(mask)] = True  # mask the upper triangle\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 9))  # create a figure and a subplot\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)  # custom color map\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask,\n",
    "    cmap=cmap,\n",
    "    center=0,\n",
    "    linewidth=0.5,\n",
    "    cbar_kws={'shrink': 0.5}\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autinsurance.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to calculate correlation between all columns and remove highly correlated ones using pandas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Create correlation matrix\n",
    "# corr_matrix = autinsurance.corr().abs()\n",
    "\n",
    "# # Select upper triangle of correlation matrix\n",
    "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# # Find features with correlation greater than 0.95\n",
    "# to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n",
    "\n",
    "# # Drop features \n",
    "# autinsurance.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "# autinsurance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop non-numerical variables\n",
    "# # numerical = list(set(autinsurance.columns) - \n",
    "# #                  set(['rowNumber', 'customerId', 'gender', 'surname','geography']))\n",
    "\n",
    "# data = autinsurance.copy()\n",
    "\n",
    "# columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "# for i in range(corr.shape[0]):\n",
    "#     for j in range(i+1, corr.shape[0]):\n",
    "#         if corr.iloc[i,j] >= 0.9:\n",
    "#             if columns[j]:\n",
    "#                 columns[j] = False\n",
    "# selected_columns = data.columns[columns]\n",
    "# data = data[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Correlation with Target Variable\n",
    "\n",
    "# autinsurance.corr()['age'].plot()\n",
    "# #autinsurance.corr()[:].plot()\n",
    "# #autinsurance.corr().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pandas scatter matrix function helps visualize the relationship between features</h3>\n",
    "Use with care though, because it is processor intensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "p=scatter_matrix(autinsurance, alpha=0.2, figsize=(30, 20), diagonal='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box and Whisker Plots\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = 20,20  # control plot size\n",
    "autinsurance.plot(kind='box', subplots=True, layout=(6,6), sharex=False, sharey=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Density Plots\n",
    "plt.rcParams['figure.figsize'] = 20,20  # control plot size\n",
    "\n",
    "autinsurance.plot(kind='density', subplots=True, layout=(6,6), sharex=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Histograms\n",
    "plt.rcParams['figure.figsize'] = 20,20  # control plot size\n",
    "\n",
    "autinsurance.hist()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.2 Scatter plot\n",
    "\n",
    "The *scatter plot* displays values of two numerical variables as *Cartesian coordinates* in 2D space. Scatter plots in 3D are also possible.\n",
    "\n",
    "Let's try out the function [`scatter()`](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.scatter.html) from the `matplotlib` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 5,5\n",
    "\n",
    "plt.scatter(autinsurance['months_as_customer'], autinsurance['age']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an uninteresting picture of two normally distributed variables. Also, it seems that these features are uncorrelated because the ellpise-like shape is aligned with the axes.\n",
    "\n",
    "There is a slightly fancier option to create a scatter plot with the `seaborn` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='age', y='total_claim_amount', \n",
    "              data=autinsurance, kind='scatter');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function [`jointplot()`](https://seaborn.pydata.org/generated/seaborn.jointplot.html) plots two histograms that may be useful in some cases.\n",
    "\n",
    "Using the same function, we can also get a smoothed version of our bivariate distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot('age', 'total_claim_amount', data=autinsurance,\n",
    "              kind=\"kde\", color=\"g\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basically a bivariate version of the *Kernel Density Plot* discussed earlier.\n",
    "\n",
    "##### 2.1.3 Scatterplot matrix\n",
    "\n",
    "In some cases, we may want to plot a *scatterplot matrix* such as the one shown below. Its diagonal contains the distributions of the corresponding variables, and the scatter plots for each pair of variables fill the rest of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `pairplot()` may become very slow with the SVG format\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "sns.pairplot(autinsurance);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of this unbalanced data, we will make sure that both our training set and testing set **maintain this ratio** of good:bad loans. This is acheived by using the `stratify` argument in the `train_test_split()` function, which was imported from the `sklearn.model_selection` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.4 Linearily Separable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt = pd.DataFrame(dict(x=X[:,0], y=X[:,1], lable = y))\n",
    "# colors  ={ 0: 'red', 1: 'blue'}\n",
    "# fig, ax = plt.subplots()\n",
    "# grouped = dt.groupby('lable')\n",
    "# for key, group in grouped:\n",
    "#     group.plot(ax=ax, kind='scatter', x ='x',y= 'y', label = key, color = colors[key])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"III\"></a>\n",
    "# III. Visual Data Analysis in with Profile Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "\n",
    "autinsurance.profile_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile = ProfileReport(autinsurance, title='Pandas Profiling Report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"IV\"></a>\n",
    "# IV. Visual Data Analysis in with SweetViz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SweetViz 2.0 Demonstration \n",
    "# Author : Kopal Rastogi\n",
    "# Created : Jan-05-2021\n",
    "# Keywords : EDA, Data Visualization,Pandas\n",
    "# Assumptions : None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "print('Importing Libraries ... ',end='')\n",
    "import sweetviz as sv\n",
    "import pandas as pd\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Analyzing data\n",
    "report=sv.analyze(autinsurance)\n",
    "\n",
    "# Generating report\n",
    "report.show_html('eda_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Playing with Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.show_html(filepath='eda_report.html',\n",
    "open_browser=True,\n",
    "layout='vertical',\n",
    "scale=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.show_notebook(w=None, \n",
    "                h=None, \n",
    "                scale=None,\n",
    "                layout='vertical',\n",
    "                filepath='sweetviz_report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autinsurance.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autinsurance['incident_date'] = pd.to_datetime(autinsurance['incident_date'],dayfirst=True, errors='coerce')\n",
    "autinsurance['policy_bind_date'] = pd.to_datetime(autinsurance['policy_bind_date'],dayfirst=True, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autinsurance['t_claim'] = autinsurance['incident_date'] - autinsurance['policy_bind_date']\n",
    "autinsurance['t_claim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete =['age', 'policy_number', 'policy_bind_date','insured_zip', 'incident_date', 'incident_city',\n",
    "       'incident_location', 'incident_hour_of_the_day','total_claim_amount','injury_claim', 'property_claim', \n",
    "        'auto_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autinsuranceV2 = autinsurance.drop(to_delete, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autinsurance.shape)\n",
    "print(autinsuranceV2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "autinsuranceV2.to_csv(\"insurance_claimsV2.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
